{
    "docs": [
        {
            "location": "/", 
            "text": "Some HPC workloads are embarrassingly parallel in the sense that they consists of a large number of independent computations performed by coordinated, but non-interacting processes.  Each of these processes performs I/O operations, potentially on a shared, parallel file system such as lustre or GPFS.\n\n\nMany (smallish) independent write operations will results in a high load for the meta data servers, which may degrade performance for all jobs on an HPC cluster.\n\n\nAlthough it is possible to tune parallel file systems for such workloads, this implies that other I/O patterns may suffer a performance degradation.  Hence it would be useful to reduce the number of meta data IOPs as much as possible.\n\n\nmem_io aims to do this, without imposing a code modification on the applications, provided they write to standard output.  To achieve this, it uses a high performance in-memory database to store the output generated by the applications.", 
            "title": "Introduction and motivation"
        }, 
        {
            "location": "/usage/", 
            "text": "Without mem_io\n\n\nConsider the following script that would perform parallel, indepedent computations on a compute node that has a total of 20 cores:\n\n\nnr_cores=20\nfor id in `seq 0 $(( ${nr_cores} - 1 ));\ndo\n    some_program  \nsome_input_${id}\n  \n  \nsome_output_${i}\n \n\ndone\nwait\n\nfor id in `seq 0 $(( ${nr_cores} - 1 ));\ndo\n    cat  \nsome_output_${i}\n  \n  \nall_output\n\ndone\n\n\n\n\nEach of the, in this case, 20 instances of the \nsome_program\n application will perform writes on standard output, which is redirected to individual files.  When the computations have been performed in parallel, the resulting individual output files are aggregated into a single file.\n\n\nNeedless to say that such an approach will lead to many I/O, and meta data operations.  This particular case is pretty mild when compared to more massively parallelized computation using the worker framework or nitro (Adaptive Computing).\n\n\nWith mem_io\n\n\nmem_io can be of help here, with a very minor modification to the workflow, and none to the application \nsome_program\n.  Consider:\n\n\nnr_cores=20\n\nmem_io_start  -nr_channels ${nr_cores}\n\nfor id in `seq 0 $(( ${nr_cores} - 1 ));\ndo\n    some_program  \nsome_input_${id}\n  |  mem_io_store  -channel_id ${id}\ndone\nwait\n\nmem_io_retrieve \n all_ouput\nmem_io_stop\n\n\n\n\nThe \nmem_io_start\n command prepares a configuration file, and starts an\nin-memory database that will be used to store the I/O.  For details, see \nStarting mem_io\n.\n\n\nRather than redirecting the standard output of processes to a file, it is piped through \nmem_io_store\n which will deal with the file I/O on behalf of the processes.  For details, see \nWriting data to mem_io\n.\n\n\nWhen all computations are done, the output is retrieved from the database by the \nmem_io_retrieve\n command, and can be redirected to a file, \nall_output\n in this example.  For details, see \nRetrieving data from mem_io\n.\n\n\nLastly, the in-memory will be shut down cleanly by \nmem_io_stop\n.  For detals, see \nStopping mem_io\n.\n\n\nNote that \nmem_io_store\n is agnostic about the nature of the processes' output, which may be text or binary.  However, for binary output, a simple concatenation of individual output will probably not result in a valid aggregated output file, so the output needs to be retrieved and redirected on a per channel basis using \nmem_io_retrieve\n \n-channel_id\n option.", 
            "title": "Use case"
        }, 
        {
            "location": "/usage/#without-mem_io", 
            "text": "Consider the following script that would perform parallel, indepedent computations on a compute node that has a total of 20 cores:  nr_cores=20\nfor id in `seq 0 $(( ${nr_cores} - 1 ));\ndo\n    some_program   some_input_${id}       some_output_${i}   \ndone\nwait\n\nfor id in `seq 0 $(( ${nr_cores} - 1 ));\ndo\n    cat   some_output_${i}       all_output \ndone  Each of the, in this case, 20 instances of the  some_program  application will perform writes on standard output, which is redirected to individual files.  When the computations have been performed in parallel, the resulting individual output files are aggregated into a single file.  Needless to say that such an approach will lead to many I/O, and meta data operations.  This particular case is pretty mild when compared to more massively parallelized computation using the worker framework or nitro (Adaptive Computing).", 
            "title": "Without mem_io"
        }, 
        {
            "location": "/usage/#with-mem_io", 
            "text": "mem_io can be of help here, with a very minor modification to the workflow, and none to the application  some_program .  Consider:  nr_cores=20\n\nmem_io_start  -nr_channels ${nr_cores}\n\nfor id in `seq 0 $(( ${nr_cores} - 1 ));\ndo\n    some_program   some_input_${id}   |  mem_io_store  -channel_id ${id}\ndone\nwait\n\nmem_io_retrieve   all_ouput\nmem_io_stop  The  mem_io_start  command prepares a configuration file, and starts an\nin-memory database that will be used to store the I/O.  For details, see  Starting mem_io .  Rather than redirecting the standard output of processes to a file, it is piped through  mem_io_store  which will deal with the file I/O on behalf of the processes.  For details, see  Writing data to mem_io .  When all computations are done, the output is retrieved from the database by the  mem_io_retrieve  command, and can be redirected to a file,  all_output  in this example.  For details, see  Retrieving data from mem_io .  Lastly, the in-memory will be shut down cleanly by  mem_io_stop .  For detals, see  Stopping mem_io .  Note that  mem_io_store  is agnostic about the nature of the processes' output, which may be text or binary.  However, for binary output, a simple concatenation of individual output will probably not result in a valid aggregated output file, so the output needs to be retrieved and redirected on a per channel basis using  mem_io_retrieve   -channel_id  option.", 
            "title": "With mem_io"
        }, 
        {
            "location": "/worker/", 
            "text": "When using mem_io together with \nworker\n, one of course has to set the mem_io password once using \nmem_io_set_password\n.  Again, this has to be done only once, see \nSetting your mem_io password\n.\n\n\nmem_io will be started from a worker prolog script, i.e.,\n\n\nmodule load mem_io\n\nmem_io_start  -nr_channels 300\n\n\n\n\nHere, the number of mem_io channels should be adapted to fit your computations.\n\n\nThe actual work is specified in the worker batch file, and the standard output is redirected to the appropriate mem_io channel.\n\n\nmodule load mem_io\n\nsome_program  \nsome_input_${id}\n  |  mem_io_store  -channel_id ${id}\n\n\n\n\nNote that the channel ID (i.e., the \n$id\n variable in this example) should be in the range 0 to number of channels - 1, inclusive.\n\n\nThe worker epilog file will retrieve the data, save it to a file \nall_output\n, and stop mem_io:\n\n\nmodule load mem_io\nmem_io_retrieve  \n  all_ouput\nmem_io_stop", 
            "title": "Using mem_io with worker"
        }, 
        {
            "location": "/mem_io_set_password/", 
            "text": "The first step is to set a password to protect access to the database.  The password will be stored in a file \n.mem_io.conf\n that is created in your home directory.  The permissions of this file will automatically be set to ensure that only you have read/write access to it.\n\n\n$ mem_io_set_password\n\n\n\n\nwill prompt you for a password, create the configuration file in your home directory with the correct permissions, and store the password in it.\n\n\nThis has to be done only once, unless you wish to change your password.\n\n\nNote  that if the \n.mem_io.conf\n file already exists, you can modify your password using the \n--force\n flag, i.e.,\n\n\n$ mem_io_set_password  --force", 
            "title": "Setting your mem_io password"
        }, 
        {
            "location": "/mem_io_start/", 
            "text": "For each job, the first thing to do is to start mem_io.  This is done using the \nmem_io_start\n command.  This will:\n\n create a run-specific mem_io configuration file,\n\n create a run-specific redis configuration file, and\n* start the redis database in the background.\n\n\nEach mem_io run is identified by a mem_io ID, which is intended to be unique.  it can either be specified on the command line using the \n-mem_io_id\n option, taken for the \nPBS_JOBID\n environment variable, or default to \ndefault\n (which is of course not unique, and will overwrite data from from previous runs with that mem_io ID).\n\n\nThe mem_io configuration file will be created in the current working directory and its name has the following pattern \nmem_io_\nmem_io_id\n.conf\n.  Since this file contains the password, care is taken that it is creaed with the correct file permissions.\n\n\nThe redis configuration file will be created in the current working directory and its name has the following pattern \nredis_\nmem_io_id\n.conf\n.  Since this file contains the password, care is taken that it is creaed with the correct file permissions.\n\n\nUpon starting mem_io, you have to specify the number of I/O channels to use.  These are independent storage entries in the database, and each store operation will append information to a specific channel, identified by a channel ID.  In practice, the number of channels is typically equal to the number of files that would have been produced by the job if mem_io were not used.\n\n\nHence, starting mem_io with, e.g., 20 channels would be accomplished by:\n\n\nmem_io_start  -nr_channels 20\n\n\n\n\nThe resulting on-disk database file will named according to the following pattern: \nmem_io_\nmem_io_id\n.rdb\n.\n\n\nNote that mem_io is only started once per job, either in the PBS user prologue file, the worker prolog script, or at the start of your PBS job script.  Hence this command \nshould never\n occor in a worker batch script!", 
            "title": "Starting mem_io"
        }, 
        {
            "location": "/mem_io_store/", 
            "text": "To store output, \nmem_io_store\n reads everything that is sent to its standard input, and stores it in the specified channel in te in-memory database.  It will retrieve all necessary information from the run specific mem_io configuration file.\n\n\nA mem_io channel ID is an integer between 0 and nr_channels - 1, inclusive.\n\n\nFor example, to store the results of an individual task or worker work item that is a computation with R:\n\n\nRscript  my_script  $var1  $var2  |  mem_io_store  -channel_id $id\n\n\n\n\nHere \n$var1\n and \n$var2\n would contain the input parameters for this task, and \n$id\n would be the mem_io channel ID.", 
            "title": "Writing data to mem_io"
        }, 
        {
            "location": "/mem_io_retrieve/", 
            "text": "To retrieve the generated output from the database, \nmem_io_retrieve\n can be used.  It will print the collected output to standard output so that it can be redirected to a file.\nOptionally, you can specify the specific channel ID to retrieve the data from.  This can be useful when the output was in some binary format, rather than ASCII text, so that concatenating the output of all channels would probably not result in a correct file.\n\n\nThis should of course only be run once when the job is completely finished.  Hence it is either in te job user epilogue, the worker epilog script, or at the very end of you PBS script.  It \nnever\n occurs in a worker batch script.\n\n\nTo retrieve all results, and saving them into a text file \ndata.txt\n, use:\n\n\nmem_io_retrieve  \n  data.txt", 
            "title": "Retrieving data from mem_io"
        }, 
        {
            "location": "/mem_io_stop/", 
            "text": "To stop mem_io, and properly shutting down the database, you simply run the \nmem_io_stop\n command.  This should of course only be run once when the job is completely finished.  Hence it is either in te job user epilogue, the worker epilog script, or at the very end of you PBS script.  It \nnever\n occurs in a worker batch script.\n\n\nmem_io_shutdown", 
            "title": "Stopping mem_io"
        }, 
        {
            "location": "/database/", 
            "text": "The database backend used by mem_io is \nredis\n.  It is an open source in-memory data structure store.  mem_io store data, as well as metadata in the database.\n\n\nThe metadata is currently limited to the number of channels, stored using the key \nmem_io_id\n:meta:nr_channels\n as a string.\n\n\nThe data is stored in redis lists, one per channel ID.  To minimize the risk of data loss, \nmem_io_store\n typically performs multiple push operations.  The keys for the data have the following format: \nmem_io_id\n:data:\nchannel_id\n.  Here,\n` is a zero-padded string representing the number of the channel.", 
            "title": "mem_io database description"
        }, 
        {
            "location": "/contact/", 
            "text": "Bug reports and feature request can be sent to the developer, \nGeert Jan Bex\n, Hasselt University, or preferably, be submitted as issues on \nGitHub\n.\n\n\nAlthough the code is publicly available on GitHub, it is nevertheless an internal tool, so no support under any form is guaranteed, although it may be provided.  It is released under the terms and conditions of the GNU General Public License, version 3.", 
            "title": "Contact information"
        }
    ]
}